# HandiTalk - Real-Time Sign Language Interpreter Mobile App

*A Final Year Project by **Harvind Nair Selvam***  
Real-time and learning-oriented American Sign Language app with two pipelines:
**Fingerspelling (TFLite on the web)** and **Phrases (CSV-feature classifier)**.
Includes **Kid Mode**, **Admin tools**, and a **Text-to-Sign** demo.

---

## âœ¨ Highlights
- **Fingerspelling (Web TFLite)**: live camera â†’ landmarks â†’ letter prediction, ~95%+ test accuracy.
- **Phrases Classifier**: temporal landmark features â†’ lightweight classifier for common phrases.
- **Text-to-Sign**: type a word/phrase to preview sign clips; shows a clean â€œNo video to previewâ€ state when unavailable.
- **Kid Mode**: larger UI, friendlier prompts, reduced controls.
- **Admin**: manage phrases and clips with Firebase Auth + Firestore + Storage.
- **Modern Frontend**: React + Vite; clean, responsive UI with sensible fallbacks.

---

## ğŸ—ï¸ Architecture (quick view)
- **Frontend**: React (Vite), JS, CSS modules
- **ML**: MediaPipe Hands (21 landmarks) â†’  
  â€¢ *Fingerspelling:* normalized features â†’ Web TFLite model  
  â€¢ *Phrases:* aggregated CSV-style features â†’ lightweight classifier
- **Backend-as-a-Service**: Firebase (Auth, Firestore, Storage)

---

## ğŸš€ Quick Start
1) Clone and install  
   `git clone https://github.com/<your-username>/handitalk.git`  
   `cd handitalk && npm install`
2) Set environment (Vite)  
   Create a `.env` with your Firebase web config:
   `VITE_FIREBASE_API_KEY`, `VITE_FIREBASE_AUTH_DOMAIN`,
   `VITE_FIREBASE_PROJECT_ID`, `VITE_FIREBASE_STORAGE_BUCKET`,
   `VITE_FIREBASE_MESSAGING_SENDER_ID`, `VITE_FIREBASE_APP_ID`.
3) Run dev server  
   `npm run dev` â†’ open the printed local URL.
4) Build & preview  
   `npm run build` then `npm run preview`.

*Tip:* If you ever see a blank screen, double-check `.env` values and that your Firestore/Storage rules allow the app to read necessary documents/files.

---

## ğŸ“ Where things live (essential folders)
- `client/src/pages/` â†’ app screens (e.g., TextToSign, KidMode, AdminPanel)
- `client/src/lib/` â†’ Firebase bootstrap, providers, and ML helpers
- `models/fingerspelling/` â†’ `handitalk_fingerspelling.tflite`, `class_names.json`
- `models/phrases/` â†’ classifier artifact(s) + `feature_spec.json`
- `docs/screens/` â†’ screenshots you reference in this README
- `docs/models/` â†’ training/evaluation images (confusion matrix, curves)

---

## ğŸ¤– Models & Evaluation (what to commit)
- Commit the **final** TFLite model + label map for Fingerspelling.
- Commit the **final** phrase classifier artifact and `feature_spec.json`.
- Put evaluation PNGs (confusion matrix, PR/ROC, loss/accuracy curves) under:
  - `docs/models/fingerspelling/â€¦`
  - `docs/models/phrases/â€¦`

If artifacts are large, use Git LFS.

---

## ğŸ§© Usage notes
- **No-clip case**: Text-to-Sign shows a clear â€œNo video to previewâ€ card.
- **Camera permissions**: Needs HTTPS on most browsers; allow camera access.
- **Optional cache**: You can enable Firestore persistence later if desired.

---

## ğŸ™Œ Acknowledgements
MediaPipe Hands â€¢ TensorFlow Lite â€¢ Open sign-language datasets and community

---

## ğŸ‘¤ Author
**Harvind Nair Selvam**  
Project: **HandiTalk - Real-Time Sign Language Interpreter Mobile App**
